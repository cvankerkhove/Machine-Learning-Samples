pred <- predict(glm.music, df.blues[-train_ind,], type = "response")
pred.bin <- as.numeric(pred >0.5)
actual.bin <- as.numeric(df.blues[-train_ind,]$label == 'classical')
mean(pred.bin == actual.bin)
####Logistic Regression####
#getting data to be fit for a single genre
df.blues <- filter(df, !(label == 'blues')) %>%
sample_n(100, replace = FALSE) %>%
mutate(label = 'other') %>%
bind_rows(filter(df, label == 'blues'))
df.blues$label <- as.factor(as.numeric(df.blues$label == 'blues'))
train_ind <- sample(1:nrow(df.blues), 0.6*nrow(df.blues))
glm.music <- glm(label~., data = df.blues[train_ind,], family = binomial)
pred <- predict(glm.music, df.blues[-train_ind,], type = "response")
pred.bin <- as.numeric(pred >0.5)
actual.bin <- as.numeric(df.blues[-train_ind,]$label == 'blues')
mean(pred.bin == actual.bin)
####Logistic Regression####
#getting data to be fit for a single genre
df.blues <- filter(df, !(label == 'blues')) %>%
sample_n(100, replace = FALSE) %>%
mutate(label = 'other') %>%
bind_rows(filter(df, label == 'blues'))
df.blues$label <- as.numeric(df.blues$label == 'blues')
train_ind <- sample(1:nrow(df.blues), 0.6*nrow(df.blues))
glm.music <- glm(label~., data = df.blues[train_ind,], family = binomial)
pred <- predict(glm.music, df.blues[-train_ind,], type = "response")
pred.bin <- as.numeric(pred >0.5)
actual.bin <- as.numeric(df.blues[-train_ind,]$label == 'blues')
mean(pred.bin == actual.bin)
####Logistic Regression####
#getting data to be fit for a single genre
df.blues <- filter(df, !(label == 'blues')) %>%
sample_n(100, replace = FALSE) %>%
mutate(label = 'other') %>%
bind_rows(filter(df, label == 'blues'))
df.blues$label <- as.factor(df.blues$label)
train_ind <- sample(1:nrow(df.blues), 0.6*nrow(df.blues))
glm.music <- glm(label~., data = df.blues[train_ind,], family = binomial)
pred <- predict(glm.music, df.blues[-train_ind,], type = "response")
pred.bin <- as.numeric(pred >0.5)
actual.bin <- as.numeric(df.blues[-train_ind,]$label == 'other')
mean(pred.bin == actual.bin)
####Logistic Regression####
#getting data to be fit for a single genre
df.blues <- filter(df, !(label == 'classical')) %>%
sample_n(100, replace = FALSE) %>%
mutate(label = 'other') %>%
bind_rows(filter(df, label == 'classical'))
df.blues$label <- as.factor(df.blues$label)
train_ind <- sample(1:nrow(df.blues), 0.6*nrow(df.blues))
glm.music <- glm(label~., data = df.blues[train_ind,], family = binomial)
pred <- predict(glm.music, df.blues[-train_ind,], type = "response")
pred.bin <- as.numeric(pred >0.5)
actual.bin <- as.numeric(df.blues[-train_ind,]$label == 'other')
mean(pred.bin == actual.bin)
####Logistic Regression####
#getting data to be fit for a single genre
df.blues <- filter(df, !(label == 'classical')) %>%
sample_n(100, replace = FALSE) %>%
mutate(label = 'other') %>%
bind_rows(filter(df, label == 'classical'))
df.blues$label <- as.factor(df.blues$label)
train_ind <- sample(1:nrow(df.blues), 0.6*nrow(df.blues))
glm.music <- glm(label~., data = df.blues[train_ind,], family = binomial)
pred <- predict(glm.music, df.blues[-train_ind,], type = "response")
pred.bin <- as.numeric(pred >0.5)
actual.bin <- as.numeric(df.blues[-train_ind,]$label == 'other')
mean(pred.bin == actual.bin)
?glmnet
library(glmnet)
library(glmnet)
?glmnet
library(tibble)
library(dplyr)
library(boot)
library(ISLR)
library(glmnet)
library(leaps)
#data pre-processing
df <- as_tibble(na.omit(Hitters))
model.pre <- lm(Salary~., df)
summary(model.pre)$adj.r.squared
outliers <- abs(rstudent(model.pre)) > 3
df.post <- df[!outliers,]
model.proc <- lm(Salary~., df.post)
summary(model.proc)$adj.r.squared
#trying no intercept model
model.noint <- lm(Salary~. + 0, df.post)
summary(model.noint)$adj.r.squared
####Subset Selection Methods####
#best subset selection, no intercept
regfit.full <- regsubsets(Salary~., data=df.post, nvmax=19, intercept=F)
reg.summary <- summary(regfit.full)
#print Cp, BIC, R^2
#pdf("cp.pdf")
plot(reg.summary$cp, xlab="p", ylab=expression(C[p]), type="b")
dev.off()
#pdf("bic.pdf")
plot(reg.summary$bic, xlab="p", ylab="BIC", type="b")
#pdf("adj_r2.pdf")
plot(reg.summary$adjr2, xlab='p', ylab=expression(R^2), type="b")
####Ridge and Lasso####
x <- model.matrix(Salary~., df.post)[,-1]
y <- df.post$Salary
grid <- 10^seq(10, -2, length=100)
grid
ridge.mod <- glmnet(x,y, alpha=0, lambda=grid, intercept=FALSE)
l2norm <- sqrt(apply(ridge.mod$beta^2, 2, sum))
l2norm
#pdf("ridge_l2.pdf)
plot(log(grid), l2norm)
####Logistic Regression####
#getting data to be fit for a single genre
df.blues <- filter(df, !(label == 'classical')) %>%
sample_n(100, replace = FALSE) %>%
mutate(label = 'other') %>%
bind_rows(filter(df, label == 'classical'))
df.blues$label <- as.factor(df.blues$label)
train_ind <- sample(1:nrow(df.blues), 0.6*nrow(df.blues))
glm.music <- glm(label~., data = df.blues[train_ind,], family = binomial)
pred <- predict(glm.music, df.blues[-train_ind,], type = "response")
pred.bin <- as.numeric(pred >0.5)
actual.bin <- as.numeric(df.blues[-train_ind,]$label == 'other')
mean(pred.bin == actual.bin)
knitr::opts_chunk$set(echo = TRUE)
####Data Cleaning and Manipulation###
music.dat <- read_csv("data.csv") %>%
tibble()
#missing values
missing <- c()
classes <- c()
for (col.name in colnames(music.dat)) {
missing[col.name] <- sum(is.na(music.dat[[col.name]]))
classes[col.name] <- class(music.dat[[col.name]])
}
knitr::opts_chunk$set(echo = TRUE)
####Logistic Regression####
#getting data to be fit for a single genre
df.blues <- filter(df, !(label == 'classical')) %>%
sample_n(100, replace = FALSE) %>%
mutate(label = 'other') %>%
bind_rows(filter(df, label == 'classical'))
setwd("~/Desktop/ORIE 4740/Final Project")
library(readr)
library(tibble)
library(dplyr)
source('Helpful_Functions.R')
####Data Cleaning and Manipulation###
music.dat <- read_csv("data.csv") %>%
tibble()
#missing values
missing <- c()
classes <- c()
for (col.name in colnames(music.dat)) {
missing[col.name] <- sum(is.na(music.dat[[col.name]]))
classes[col.name] <- class(music.dat[[col.name]])
}
#Confirmed no missing data
sapply(music.dat, class)
#classes of each column
df <- select(music.dat, -filename)
df$label <- as.factor(df$label)
View(d)
View(df)
####Logistic Regression####
#getting data to be fit for a single genre
df.blues <- filter(df, !(label == 'classical')) %>%
sample_n(100, replace = FALSE) %>%
mutate(label = 'other') %>%
bind_rows(filter(df, label == 'classical'))
df.blues$label <- as.factor(df.blues$label)
train_ind <- sample(1:nrow(df.blues), 0.6*nrow(df.blues))
glm.music <- glm(label~., data = df.blues[train_ind,], family = binomial)
pred <- predict(glm.music, df.blues[-train_ind,], type = "response")
pred.bin <- as.numeric(pred >0.5)
actual.bin <- as.numeric(df.blues[-train_ind,]$label == 'other')
mean(pred.bin == actual.bin)
setwd("~/Desktop/ORIE 4740/Lab_4")
library(tibble)
library(dplyr)
library(boot)
library(ISLR)
library(glmnet)
library(leaps)
#data pre-processing
df <- as_tibble(na.omit(Hitters))
model.pre <- lm(Salary~., df)
summary(model.pre)$adj.r.squared
outliers <- abs(rstudent(model.pre)) > 3
df.post <- df[!outliers,]
model.proc <- lm(Salary~., df.post)
summary(model.proc)$adj.r.squared
#trying no intercept model
model.noint <- lm(Salary~. + 0, df.post)
summary(model.noint)$adj.r.squared
####Subset Selection Methods####
#best subset selection, no intercept
regfit.full <- regsubsets(Salary~., data=df.post, nvmax=19, intercept=F)
reg.summary <- summary(regfit.full)
#print Cp, BIC, R^2
#pdf("cp.pdf")
plot(reg.summary$cp, xlab="p", ylab=expression(C[p]), type="b")
source('~/Desktop/ORIE 4740/Lab_4/ORIE 4740 Lab4.R', echo=TRUE)
setwd("~/Desktop/ORIE 4740/Lab_8")
library(e1071)
mushroom <- read.csv("mushroom.csv")
setwd("~/Desktop/ORIE 4740/Lab_8")
library(e1071)
mushroom <- read.csv("mushroom.csv")
library(e1071)
mushroom <- read.csv("mushroom.csv")
distm <- as.dist(hamming.distance(as.matrix(mushroom)[, 2:14]))
hc.complete <- hclust(distm, method="complete")
plot(hc.copmlete, main="Complte Linkage", cex=0.9)
plot(hc.complete, main="Complte Linkage", cex=0.9)
hc.cut <- cutree(hc.complete, 8)
plot(hc.cut)
#plotting and cutting tree
plot(hc.complete, main="Complete Linkage", cex=0.9)
plot(hc.cut, main = 'Complete Linkage', cex=0.9)
table(mushroom[,1], hc.cut)
#cutting dendrogram to have 2 clusters
hc.cut <- cutree(hc.complete, 2)
table(mushroom[,1], hc.cut)
mushroom[,1]
mushroom
mushroom[1,]
hc.cut
table(mushroom[,1], hc.cut)
#creating data with dummy variables to replace categorical ones
dat <- model.matrix(~., data=mushroom[,2:14])[,-1]
dat
mushroom
?model.matrix
######
#Hieracrhcial clustering using Euclidean distance among the dummy vars
#creating data with dummy variables to replace categorical ones
dat <- model.matrix(~., data=mushroom[,2:14])[,-1]
hc.complete2 <- hclust(dist(dat), method="complete")
hc.cut2 <- cutree(hc.complete2, 8)
table(mushroom[,1], hc.cut2)
####Principal Component Analysis####
names(iris)
View(iris)
tibble(iris)
iris_means <- apply(iris[, -5], 2, mean)
iris_means
iris[, -5]
iris_means <- apply(iris[, -5], 1, mean)
iris_means
iris_means <- apply(iris[, -5], 2, mean)
iris_vars <- apply(iris[,-5], 2, var)
#PCA
pr.out <- prcomp(iris[,-5], scale=TRUE)
#PCA out values
pr.out$center
iris_means
pr.out$scale
iris_vars
0.82^2
0.8280661^2
pr.out$rotation
biplot(pr.out, scale=0)
#checking variance explained by each component
pr.var <- pr.out$sdev^2
pr.out$sdev
pve <- pr.var / sum(pr.var)
pve
sum(pve)
plt(pve, xlab="Principal Component", ylab='Proportion of Variance Explained', ylim=c(0,1)
, type = 'b')
plot(pve, xlab="Principal Component", ylab='Proportion of Variance Explained', ylim=c(0,1)
, type = 'b')
plot(cumsum(pve), xlab='Principal Component', ylab="Cumulative Proportion of Variance Explained",
ylim=c(0,1), type='b')
###
#Principal Component Regression
library(pls)
install.packages(pls)
install.packages('pls')
###
#Principal Component Regression
library(pls)
library(ISLR)
Hitters <- Hitters
sum(is.na(Hitters))
Hitters=na.omit(Hitters)
set.seed(2)
train = sample(1:nrow(Hitters), nrow(Hitters)*2/3)
Hitters.test = Hitters[-train,]
set.seed(2)
pcr.fit=pcr(Salary~., data=Hitters, scale=TRUE, validation="CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type= 'MSEP')
#predicting on test data
pcr.pred = predict(pcr.fit, Hitters.test, ncomp = 6)
mean((pcr.pred - Hitters.test$Salary)^2)
#PCR final fit with ncomp=6 on full data set
pcr.fit.final = pcr(Salary~., data= Hitters, scale = TRUE, ncomp=6)
summary(pcr.fit.final)
####Take Home Questions####
library(MASS)
Boston
is.na(Boston)
sum(is.na(Boston))
set.seed(12)
sum(is.na(Boston))
set.seed(12)
train = sample(1:nrow(Boston), nrow(Boston)*0.8)
Boston.test = Boston[-train,]
#(i)
pcr.fit <- pcr(medv~., data=Boston, scale=TRUE, validation='CV')
summary(pcr.fit)
validationplot(pcr.fit, val.type= 'MSEP')
#predicting on test data
pcr.pred = predict(pcr.fit, Boston.test, ncomp = 5)
mean((pcr.pred - Boston.test$medv)^2)
pcr.fit.train <- pcr(medv~., data=Boston[train,], scale=TRUE)
#predicting on test data
pcr.pred = predict(pcr.fit.train, Boston.test, ncomp = 5)
mean((pcr.pred - Boston.test$medv)^2)
####Take Home Questions####
library(MASS)
sum(is.na(Boston))
set.seed(12)
train = sample(1:nrow(Boston), nrow(Boston)*0.8)
Boston.test = Boston[-train,]
#(i)
pcr.fit <- pcr(medv~., data=Boston, scale=TRUE, validation='CV')
summary(pcr.fit)
validationplot(pcr.fit, val.type= 'MSEP')
pcr.fit.train <- pcr(medv~., data=Boston[train,], scale=TRUE, ncomp=5)
#predicting on test data
pcr.pred = predict(pcr.fit.train, Boston.test, ncomp = 5)
mean((pcr.pred - Boston.test$medv)^2)
#PCR final fit with ncomp=6 on full data set
pcr.fit.final = pcr(Salary~., data= Boston, scale = TRUE, ncomp=5)
#PCR final fit with ncomp=6 on full data set
pcr.fit.final = pcr(medv~., data= Boston, scale = TRUE, ncomp=5)
summary(pcr.fit.final)
####Data Cleaning and Manipulation###
music.dat <- read_csv("data.csv") %>%
tibble()
View(music.dat)
levels(music.dat$label)
view(music.dat)
setwd("~/Desktop/ML Algos Samples")
library(tibble)
library(dplyr)
library(boot)
library(ISLR)
library(glmnet)
library(leaps)
#data pre-processing
df <- as_tibble(na.omit(Hitters))
model.pre <- lm(Salary~., df)
summary(model.pre)$adj.r.squared
outliers <- abs(rstudent(model.pre)) > 3
df.post <- df[!outliers,]
model.proc <- lm(Salary~., df.post)
summary(model.proc)$adj.r.squared
#trying no intercept model
model.noint <- lm(Salary~. + 0, df.post)
summary(model.noint)$adj.r.squared
####Subset Selection Methods####
#best subset selection, no intercept
regfit.full <- regsubsets(Salary~., data=df.post, nvmax=19, intercept=F)
reg.summary <- summary(regfit.full)
#print Cp, BIC, R^2
#pdf("cp.pdf")
plot(reg.summary$cp, xlab="p", ylab=expression(C[p]), type="b")
dev.off()
#pdf("bic.pdf")
plot(reg.summary$bic, xlab="p", ylab="BIC", type="b")
#pdf("adj_r2.pdf")
plot(reg.summary$adjr2, xlab='p', ylab=expression(R^2), type="b")
####Ridge and Lasso####
x <- model.matrix(Salary~., df.post)[,-1]
y <- df.post$Salary
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x,y, alpha=0, lambda=grid, intercept=FALSE)
l2norm <- sqrt(apply(ridge.mod$beta^2, 2, sum))
#pdf("ridge_l2.pdf)
plot(log(grid), l2norm)
#Split data into training data set and test data
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
y.train <- y[train]
y.test <- y[-train]
##RIDGE
#model fit on the training set
ridge.mod <- glmnet(x[train,], y.train, alpha=0, lambda=grid, intercept=FALSE)
set.seed(1)
cv.ridge <- cv.glmnet(x[train,], y.train, nfolds = 10, lambda=grid, alpha=0, intercept=FALSE)
lbest <- cv.ridge$lambda.min
ridge.pred <- predict(ridge.mod, s=lbest, newx=x[-train,])
ridge.mse <- mean((ridge.pred - y.test)^2)
print(paste("Ridge MSE (Hitters): ", ridge.mse))
print(paste("Optimal lambda (Hitters): ", lbest))
plot(cv.ridge)
##LASSO
#model fit on training set
lasso.mod <- glmnet(x[train,], y[train], alpha=1, lambda=grid, intercept = FALSE)
#Use 10-fold cross-validation to choose lambda
set.seed(1)
cv.lasso <- cv.glmnet(x[train,], y[train], alpha=1, lambda=grid, intercept=FALSE)
lbest <- cv.lasso$lambda.min
lasso.pred <- predict(lasso.mod, s=lbest, newx=x[-train,])
lasso.mse <- mean((lasso.pred - y.test)^2)
print(paste("LASSO MSE (Hitters: ", lbest))
print(paste("Optimal lambda (hitters): ", lasso.mse))
plot(cv.lasso)
setwd("~/Desktop/ML Algos Samples")
library(ISLR)
library(boot)
library(splines)
library(gam)
####-Generalized Additive Models
set.seed(1)
#allows access to these variables
attach(Wage)
#GAM 1 (natural splines)
gam1 <- lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
#GAM 2(smoothing splines)
gam2 <- gam(wage~s(year,4)+s(age,5)+education,data=Wage)
par(mfrow=c(2,3))
plot.Gam(gam1, se=TRUE, col="blue")
plot(gam2, se=TRUE, col="red")
#compare GAMs via ANOVA
gam.m1 <- gam(wage~s(age,5) + education, data=Wage)
gam.m2 <- gam(wage~year + s(age,5) + education, data=Wage)
gam.m3 <- gam(wage~s(year, 4) + s(age, 5) + education, data=Wage)
gamTest <- anova(gam.m1, gam.m2, gam.m3, test="F")
print(gamTest)
#logistic regression with GAMs
gam.logreg <- gam(I(wage>250)~ year + s(age,df=5) + education, family=binomial,
data=Wage)
par(mfrow=c(1,3))
plot(gam.logreg, se=T, col='blue')
#inspecting data
levels(education[I(wage >250)])
table(education, I(wage>250))
#fixing gam.logreg
gam.fix <- gam(I(wage>250)~year + s(age, df=5) + education, family=binomial,
data=Wage, subset=(education != "1. < HS Grad"))
par(mfrow=c(1,3))
plot(gam.fix, se=T, col="blue")
####- Backfitting with multiple linear regression
set.seed(1)
X1 = rnorm(100)
X2 = rnorm(100)
X3 = sin(X2)
eps = rnorm(100, sd = 0.01)
Y=10+0.8 *X1+6*X2+ 1.1*X3+eps
df <- data.frame(Y,X1, X2, X3)
#initalize B1
b1 <- 3
#iteration 1
lmFix1 <- lm(Y - b1 * X1 ~ ., data=df)
summary(lmFix1)
b2 <- lmFix1$coefficients['X2']
print(b2)
#iteration 2
lmFix2 <- lm(Y - b2 * X2 ~ ., data=df)
summary(lmFix2)
print(lmFix2$coefficients['(Intercept)'])
print(lmFix2$coefficients['X3'])
b3 <- lmFix2$coefficients['X3']
#iteration 3
lmFix3 <- lm(Y - b3 * X3 ~., data=df)
summary(lmFix3)
b1 <- lmFix3$coefficients['X1']
lmFix3$coefficients
#part vi, loop for iterations
coef.iter <- function(n, v, plt = FALSE){
#backfitting with multiple linear regression
#n: number of iterations
#v: starting value for B1
B1.list <- rep(NA, n)
B2.list <- rep(NA, n)
B3.list <- rep(NA, n)
B0.list <- rep(NA, n)
#starting value
b1 = v
for (i in 1:n){
#fixing B1
lmFix1 <- lm(Y - b1 * X1 ~ ., data=df)
b2 <- lmFix1$coefficients['X2']
#fixing B2
lmFix2 <- lm(Y - b2 * X2 ~ ., data=df)
b3 <- lmFix2$coefficients['X3']
#fixing b3
lmFix3 <- lm(Y - b3 * X3 ~., data=df)
b1 <- lmFix3$coefficients['X1']
B1.list[i] <- b1
B2.list[i] <- b2
B3.list[i] <- b3
B0.list[i] <- lmFix3$coefficients['(Intercept)']
}
if (plt) {
par(mfrow=c(2,2))
plot(1:n,B1.list)
plot(1:n, B2.list)
plot(1:n, B3.list)
plot(1:n, B0.list)
}
#returning final values
return (c(b1,b2,b3))
}
backfit <- coef.iter(10, 3, plt = TRUE)
print(backfit)
